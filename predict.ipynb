{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import OrderedDict\t\n",
    "from PIL import Image\n",
    "import seaborn as sb\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_paser():\n",
    "    paser = argparse.ArgumentParser(description='trainer file')\n",
    "    paser.add_argument('--data_dir', type=str, default='flowers', help='dataset directory')\n",
    "    paser.add_argument('--gpu', type=bool, default='True', help='True: gpu, False: cpu')\n",
    "    paser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "    paser.add_argument('--epochs', type=int, default=10, help='num of epochs')\n",
    "    paser.add_argument('--arch', type=str, default='densenet201', help='architecture')\n",
    "    paser.add_argument('--hidden_layers', type=int, default=[25088, 102], help='hidden layers for layer')\n",
    "    paser.add_argument('--save_dir', type=str, default='checkpoint.pth', help='save train model to a file')\n",
    "    args = paser.parse_args()\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_model(arch):\n",
    "   # Load pretrained_network\n",
    "    print('Please only select densenet201 or alexnet as pretrained model. By default densenet201 is selected!')\n",
    "    if arch == None or arch == 'densenet201':\n",
    "        load_model = models.densenet201(pretrained = True)\n",
    "#load_model.name = 'densenent201'\n",
    "        print('Current model: densenet201 loaded')\n",
    "    else:\n",
    "        load_model = models.alexnet (pretrained = True)\n",
    "        print('Current model: alexnet loaded')\n",
    "#model.name = alexnet\n",
    "\n",
    "    return load_model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-53da62bd2851>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-53da62bd2851>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def loading_value(file, arch)\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a function that loads a checkpoint and rebuilds the model\n",
    "def loading_value(file, arch)\n",
    "\n",
    "# Load checkpoint according to filename/filepath\n",
    "checkpoint = torch.load(file)\n",
    "\n",
    "# Load model corresponding to arch - type\n",
    "model = what_model(arch)\n",
    "\n",
    "# Freeze parameters\t\n",
    "for param in model.parameters():\n",
    "param.requires_grad = False\t\n",
    "\n",
    "  \n",
    "    model.classifier =  checkpoint['classifier']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "     \n",
    "         \n",
    "                               \n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#model = loading_value('checkpoint.pth')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \n",
    "    # Open image from the image_path\n",
    "    im = Image.open(image_path)\n",
    "    \n",
    "    # Preprocess using a the approach from the previous transformations\n",
    "    \n",
    "    preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
    "    \n",
    "    fin_im = preprocess(im)\n",
    "\n",
    "    return np.array(fin_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls flowers/test/81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''#model: pre-trained model\n",
    "    # image path\n",
    "    \n",
    "    # TODO: Implement the code to predict the class from an image file\n",
    "     # Checking if the 'GPU' is available to pass it for the device variable, and if it's not, pass the 'CPU'\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    # Move model to the device\n",
    "    model.to(device)\n",
    "    # Model in inference mode, dropout is off\n",
    "    model.eval()\n",
    "    \n",
    "    image = process_image(image_path)\n",
    "    #print(image.size()) >>> torch.Size([3, 244, 244])\n",
    "    image.unsqueeze_(0) \n",
    "    #print(image.size()) >>> torch.Size([1, 3, 244, 244])\n",
    "    \n",
    "    # Move image tensors to the device.\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Turn off gradients for testing saves memory and computations, so will speed up inference.\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the network to get the outputs.\n",
    "        prediction = model.forward(image)\n",
    "    # Take exponential to get the probabilities from log softmax output.\n",
    "    ps = torch.exp(prediction)\n",
    "    # The most likely (topk) predicted prbabilities with their indices.\n",
    "    probs, top_k_indices = ps.topk(topk)\n",
    "    \n",
    "    # Extracting the classes from the indices.\n",
    "    classes = []\n",
    "    for indice in top_k_indices.gpu()[0]:  #cpu\n",
    "        classes.append(list(model.class_to_idx)[indice.numpy()]) # Here we take the class from the index.\n",
    "    \n",
    "    return probs.gpu()[0].numpy(), classes    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main()\n",
    "    args = args_paser()\n",
    "    \n",
    "    data_dir = 'flowers'\n",
    "    train_dir = data_dir + '/train'\n",
    "    valid_dir = data_dir + '/valid'\n",
    "    test_dir = data_dir + '/test'\n",
    "    \n",
    "        \n",
    "    with open('cat_to_name.json', 'r') as f:\n",
    "        cat_to_name = json.load(f)\n",
    "        \n",
    "    print('Checkpoint')\n",
    "    \n",
    "    optimizer, load_model = load_checkpoint('checkpoint.pth', args.arch)\n",
    "    print(\"Predicting Image Probability and Class...\")\n",
    "    probs, classes = predict(device, img_path, model, top_k)\n",
    "    print(probs)\n",
    "    print(classes)\n",
    "    print(\"WoW!\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
